{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42aa99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "from glob import glob\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import networks \n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ae73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def srganPath(p):\n",
    "    return p + \"SRGAN_generator.pth\"\n",
    "def metaSrganPath(p):\n",
    "    return p + \"MetaSRGAN_generator.pth\"\n",
    "def autoencoderPath(p):\n",
    "    return p + \"DNO_Autoencoder.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfcdbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample(imgs, path):\n",
    "    batch_size = imgs[0].shape[0]\n",
    "    grid_imgs = []\n",
    "    for img_seq in imgs:\n",
    "        grid_imgs.append(make_grid(img_seq[0:batch_size], nrow=1, normalize=False))\n",
    "    imgs_all = torch.cat(grid_imgs, dim=-1)\n",
    "    sample_num = len(glob(path + \"MetaSRGAN_Inference_*.png\"))\n",
    "    save_image(imgs_all, path + (\"MetaSRGAN_Inference_%d.png\" % sample_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2bb313",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"../CheckPoints/\"\n",
    "src              = \"../Samples/Example5/\"\n",
    "dst              = \"../Outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b671f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaSrgan = networks.GeneratorResNet().cuda()\n",
    "srgan     = networks.GeneratorSlowNet().cuda()\n",
    "encoder   = networks.MetaAutoencoderTail().cuda()\n",
    "\n",
    "metaSrgan.load_state_dict(torch.load(metaSrganPath(saved_model_path)))\n",
    "srgan.load_state_dict(torch.load(srganPath(saved_model_path)))\n",
    "encoder.load_state_dict(torch.load(autoencoderPath(saved_model_path)))\n",
    "\n",
    "metaSrgan.eval()\n",
    "srgan.eval()\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c42ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "img    = Image.open(src + \"Image.png\").convert(mode=\"RGB\")\n",
    "normal = Image.open(src + \"Normal.png\").convert(mode=\"RGB\")\n",
    "depth  = Image.open(src + \"Depth.png\").convert(mode=\"RGB\")\n",
    "obj    = Image.open(src + \"Object.png\").convert(mode=\"RGB\")\n",
    "\n",
    "img    = tensor_transform(img).cuda()[None,:]\n",
    "normal = tensor_transform(normal).cuda()[None,:]\n",
    "depth  = tensor_transform(depth).cuda()[None,:]\n",
    "obj    = tensor_transform(obj).cuda()[None,:]\n",
    "\n",
    "normal = srgan(normal)\n",
    "depth  = srgan(depth)\n",
    "obj    = srgan(obj)\n",
    "\n",
    "metas  = torch.cat((depth[:,0:1], normal, obj[:,0:1]), dim=1)\n",
    "\n",
    "metaVec = encoder(metas)\n",
    "\n",
    "inference = metaSrgan(img, metaVec)\n",
    "\n",
    "save_sample([inference], dst)\n",
    "\n",
    "print(\"Inference complete in %f seconds\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
